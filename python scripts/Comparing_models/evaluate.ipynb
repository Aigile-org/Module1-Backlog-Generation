{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Project Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Obtaining dependency information for sentence-transformers from https://files.pythonhosted.org/packages/05/89/7eb147a37b7f31d3c815543df539d8b8d0425e93296c875cc87719d65232/sentence_transformers-3.4.1-py3-none-any.whl.metadata\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Obtaining dependency information for transformers<5.0.0,>=4.41.0 from https://files.pythonhosted.org/packages/b6/1a/efeecb8d83705f2f4beac98d46f2148c95ecd7babfb31b5c0f1e7017e83d/transformers-4.48.3-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     ----------------- -------------------- 20.5/44.4 kB 330.3 kB/s eta 0:00:01\n",
      "     ----------------------------------- -- 41.0/44.4 kB 393.8 kB/s eta 0:00:01\n",
      "     -------------------------------------- 44.4/44.4 kB 367.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\ahma3\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\ahma3\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ahma3\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\ahma3\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.11.1)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Obtaining dependency information for huggingface-hub>=0.20.0 from https://files.pythonhosted.org/packages/ea/da/6c2bea5327b640920267d3bf2c9fc114cfbd0a5de234d81cda80cc9e33c8/huggingface_hub-0.28.1-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\ahma3\\anaconda3\\lib\\site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ahma3\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.9.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Obtaining dependency information for fsspec>=2023.5.0 from https://files.pythonhosted.org/packages/e2/94/758680531a00d06e471ef649e4ec2ed6bf185356a7f9fbfbb7368a40bd49/fsspec-2025.2.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2025.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\ahma3\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ahma3\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ahma3\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ahma3\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\ahma3\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\ahma3\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ahma3\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ahma3\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ahma3\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ahma3\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.7.9)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Obtaining dependency information for tokenizers<0.22,>=0.21 from https://files.pythonhosted.org/packages/44/69/d21eb253fa91622da25585d362a874fa4710be600f0ea9446d8d0217cec1/tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Obtaining dependency information for safetensors>=0.4.1 from https://files.pythonhosted.org/packages/86/ca/aa489392ec6fb59223ffce825461e1f811a3affd417121a2088be7a5758b/safetensors-0.5.2-cp38-abi3-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\ahma3\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ahma3\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ahma3\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ahma3\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ahma3\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ahma3\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ahma3\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ahma3\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "   ---------------------------------------- 0.0/275.9 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 41.0/275.9 kB ? eta -:--:--\n",
      "   ---------- ---------------------------- 71.7/275.9 kB 991.0 kB/s eta 0:00:01\n",
      "   -------------------- ------------------- 143.4/275.9 kB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------- -------------- 174.1/275.9 kB 958.1 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 225.3/275.9 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 275.9/275.9 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "   ---------------------------------------- 0.0/464.1 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 41.0/464.1 kB 2.0 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 143.4/464.1 kB 1.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 225.3/464.1 kB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 317.4/464.1 kB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 399.4/464.1 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 464.1/464.1 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/9.7 MB 2.4 MB/s eta 0:00:04\n",
      "    --------------------------------------- 0.2/9.7 MB 2.8 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.3/9.7 MB 2.6 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.5/9.7 MB 2.6 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.6/9.7 MB 2.7 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.7/9.7 MB 2.7 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.9/9.7 MB 2.7 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.0/9.7 MB 2.7 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.1/9.7 MB 2.7 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.3/9.7 MB 2.8 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.5/9.7 MB 2.9 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.6/9.7 MB 3.0 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.8/9.7 MB 3.0 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.0/9.7 MB 3.0 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.1/9.7 MB 3.1 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.3/9.7 MB 3.1 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.5/9.7 MB 3.1 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.6/9.7 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.8/9.7 MB 3.2 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.0/9.7 MB 3.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 3.1/9.7 MB 3.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 3.3/9.7 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.5/9.7 MB 3.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.7/9.7 MB 3.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.8/9.7 MB 3.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.0/9.7 MB 3.3 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.2/9.7 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.4/9.7 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.6/9.7 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.7/9.7 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.9/9.7 MB 3.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.1/9.7 MB 3.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.3/9.7 MB 3.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.5/9.7 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.7/9.7 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.8/9.7 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.0/9.7 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.2/9.7 MB 3.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.4/9.7 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.5/9.7 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.6/9.7 MB 3.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.7/9.7 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.9/9.7 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.0/9.7 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.1/9.7 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.2/9.7 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.4/9.7 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.5/9.7 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.6/9.7 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.8/9.7 MB 3.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.0/9.7 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.7/9.7 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.0/9.7 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.0/9.7 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.5/9.7 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.7/9.7 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 3.1 MB/s eta 0:00:00\n",
      "Downloading fsspec-2025.2.0-py3-none-any.whl (184 kB)\n",
      "   ---------------------------------------- 0.0/184.5 kB ? eta -:--:--\n",
      "   ---------------------------------------  184.3/184.5 kB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 184.5/184.5 kB 3.7 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "   ---------------------------------------- 0.0/303.8 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 194.6/303.8 kB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 303.8/303.8 kB 3.8 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.2/2.4 MB 4.1 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.4/2.4 MB 4.0 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.5/2.4 MB 4.2 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.5/2.4 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.9/2.4 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.9/2.4 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.1/2.4 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.2/2.4 MB 3.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.6/2.4 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.8/2.4 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.9/2.4 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.1/2.4 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.4/2.4 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 3.5 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, fsspec, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.3.2\n",
      "    Uninstalling safetensors-0.3.2:\n",
      "      Successfully uninstalled safetensors-0.3.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.15.1\n",
      "    Uninstalling huggingface-hub-0.15.1:\n",
      "      Successfully uninstalled huggingface-hub-0.15.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.2\n",
      "    Uninstalling tokenizers-0.13.2:\n",
      "      Successfully uninstalled tokenizers-0.13.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.32.1\n",
      "    Uninstalling transformers-4.32.1:\n",
      "      Successfully uninstalled transformers-4.32.1\n",
      "Successfully installed fsspec-2025.2.0 huggingface-hub-0.28.1 safetensors-0.5.2 sentence-transformers-3.4.1 tokenizers-0.21.0 transformers-4.48.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~pencv-python (C:\\Users\\ahma3\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~pencv-python (C:\\Users\\ahma3\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~pencv-python (C:\\Users\\ahma3\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution ~pencv-python (C:\\Users\\ahma3\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution ~pencv-python (C:\\Users\\ahma3\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution ~pencv-python (C:\\Users\\ahma3\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution ~pencv-python (C:\\Users\\ahma3\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution ~pencv-python (C:\\Users\\ahma3\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ultralytics 8.3.37 requires opencv-python>=4.6.0, which is not installed.\n",
      "s3fs 2023.4.0 requires fsspec==2023.4.0, but you have fsspec 2025.2.0 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution ~pencv-python (C:\\Users\\ahma3\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~pencv-python (C:\\Users\\ahma3\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~pencv-python (C:\\Users\\ahma3\\AppData\\Roaming\\Python\\Python311\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ahma3\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ahma3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Initialize the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "mvp=\"The MVP aims to develop a chatbot capable of real-time analysis, providing insights into supplier offerings. The final LLM (Large Language Model) assistant will help managers make informed decisions based on cost analysis, ensuring profitable deals. The first phase targets a release date of July 23rd. Technical Approach PDF Extraction: Obtain PDFs from different vendors of each supplier and extract the content. Translation: Translate the extracted data into English for internal processing. Data Structuring: Convert the data into a structured format suitable for use with a Large Language Model (LLM). Storage: Store the structured data in a vector database like MongoDB. LLM Integration: Utilize the structured data to enable the LLM to generate accurate and relevant responses through well-crafted prompts or queries. Demo: Aim to deliver a functional demo by the third week of July. Use Cases or Query Prompts Ingredient Requirement for Lasagna: Determine the required ingredients and their quantities for making 10 dishes of lasagna, along with the current rates. Supplier Rate Comparison for Pasta: Identify which supplier offers the best rates and deals for making pasta this month. Recipe and Cost Analysis for Lasagna: Retrieve the recipe for lasagna, list all necessary ingredients, and calculate the cost of preparing 20 plates for the next day. Future Prospects API Development: Provide APIs for the team to upload PDFs and perform detailed insights on historical data and various offers. Enhanced Data Analysis: Enable deeper analysis of historical data and trends to optimize procurement and cost management. This approach ensures a streamlined process for managing supplier data, facilitating better decision-making through advanced data analysis and LLM integration.\"\n",
    "vision=\"Aligning more than 500 different vendors and API solutions is difficult. The current system struggles to manage 500 suppliers, each with different vendors, making it difficult to centralize and compare prices. The complexity of tracking menu costs, resource costs, price changes, and different vendors creates significant challenges. A unified platform is needed to streamline this process, allowing for the extraction and analysis of data from various vendor PDFs. This platform would store the data in a vector format to provide better insights. Nick will provide more PDFs of different vendors to run the experiment smoothly. The current system struggles to manage 500 suppliers, each with different vendors, making it difficult to centralize and compare prices.\"\n",
    "# General project description\n",
    "project_description = mvp+vision\n",
    "\n",
    "# Tokenize the project description into sentences\n",
    "nltk.download('punkt')\n",
    "sentences = sent_tokenize(project_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama3 70 billion Average Similarity: 0.5817118525505066\n",
      "Llama3 8 billion Average Similarity: 0.5613284543866203\n",
      "Llama3 70 billion_8192 Average Similarity: 0.7231890048299517\n",
      "Llama3 8 billion_8192 Average Similarity: 0.700667409102122\n"
     ]
    }
   ],
   "source": [
    "from user_stories_llama_8b_instant import user_stories\n",
    "from user_stories_llama_70b_versatile import user_stories_70b\n",
    "from user_stories_llama3_70b_8192 import user_stories_70b_8192\n",
    "from user_stories_llama3_8b_8192 import user_stories_8b_8192\n",
    "\n",
    "#Calculate average similarity for a model\n",
    "def cosine_average_similarity(user_stories):\n",
    "    scores = []\n",
    "    for item in user_stories:\n",
    "        # print(item)\n",
    "        key,story,epic,description=item.items()\n",
    "        story_embedding = model.encode(story)\n",
    "        desc_embeddings = model.encode(sentences)\n",
    "        # Calculate simialrity between us and all sentences\n",
    "        # .max(): We take the single highest score from the results. This answers the question:\n",
    "        #   \"What sentence in the original document is this user story MOST similar to?\"\n",
    "        #   This is a good metric for checking if the story is \"grounded\" in at least one\n",
    "        #   part of the original requirement\n",
    "        similarity = util.cos_sim(story_embedding, desc_embeddings).max()\n",
    "        # The result is a tensor object containing a single number. .item() extracts\n",
    "        # this number as a standard Python float.\n",
    "        scores.append(similarity.item())\n",
    "    # We calculate the average of all the collected scores to get a final evaluation metric\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "# Calculate and print average similarity scores for each model\n",
    "average_llama_70b = cosine_average_similarity(user_stories_70b)\n",
    "average_llama_8b = cosine_average_similarity(user_stories)\n",
    "average_llama_70b_8192 = cosine_average_similarity(user_stories_70b_8192)\n",
    "average_llama_8b_8192 = cosine_average_similarity(user_stories_8b_8192)\n",
    "\n",
    "print(f\"Llama3 70 billion Average Similarity: {average_llama_70b}\")\n",
    "print(f\"Llama3 8 billion Average Similarity: {average_llama_8b}\")\n",
    "print(f\"Llama3 70 billion_8192 Average Similarity: {average_llama_70b_8192}\")\n",
    "# llama-guard-3-8b -> gave User story not provided\n",
    "print(f\"Llama3 8 billion_8192 Average Similarity: {average_llama_8b_8192}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
